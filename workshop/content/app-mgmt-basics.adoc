## アプリケーション管理の基本
このモジュールでは、`oc` ツールを使用してサンプルアプリケーションをデプロイし、OpenShift Container Platform上での中心概念、基本オブジェクト、アプリケーション管理の基本について学習します。

### OpenShiftの中心概念
OpenShiftの管理者として、アプリケーションに関連するいくつかのコアとなるビルディングブロックを理解することは重要です。これらのビルディングブロックを理解することで、プラットフォーム上でのアプリケーション管理の全体像をよりよく理解することができます。

### Projects
*Project* は一種の「バケツ」です。これは、ゆべてのユーザーのリソースが存在するメタ構造です。管理の観点からは、各Projectはテナントのように考えることができます。Projectにはアクセスできる複数のユーザーがいるかもしれませんし、ユーザーは複数のProjectにアクセスできるかもしれません。技術的に言えば、リソースはユーザーが所有しているのではなく、Projectが所有しています。ユーザーを削除しても、作成されたリソースには影響しません。

この演習では、まず、いくつかのリソースを保持するProjectを作成します。

[source,bash,role="execute"]
----
oc new-project app-management
----

### サンプルアプリケーションのデプロイ
`new-app` コマンドは、OpenShiftに物事を実行するよう指示するシンプルな方法です。
このコマンドに幅拾い入力事項のうちのひとつを与えるだけで、OpenShiftは何をすべきかを判断します。
ユーザーはこのコマンドを使ってOpenShiftで既存のイメージを起動させたり、ソースコードをビルドしてデプロイしたり、テンプレートをインスタンス化したりするのが一般的です。

次のようにDockerhub上に存在する特定のイメージを起動します。

[source,bash,role="execute"]
----
oc new-app quay.io/thoraxe/mapit
----

出力は以下のようになります。

----
--> Found Docker image 7ce7ade (20 months old) from quay.io for "quay.io/thoraxe/mapit"

    * An image stream tag will be created as "mapit:latest" that will track this image
    * This image will be deployed in deployment config "mapit"
    * Ports 8080/tcp, 8778/tcp, 9779/tcp will be load balanced by service "mapit"
      * Other containers can access this service through the hostname "mapit"

--> Creating resources ...
    imagestream.image.openshift.io "mapit" created
    deploymentconfig.apps.openshift.io "mapit" created
    service "mapit" created
--> Success
    Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
     'oc expose svc/mapit'
    Run 'oc status' to view your app.
----

このコマンドの出力でOpenShiftがいくつかのリソースを自動的に作成したことがわかります。作成されたリソースを少し時間をかけて調べてみましょう。

`new-app` の機能の詳細については、 `oc new-app -h` を実行してそのヘルプメッセージを見てください。

### Pods

.OpenShift Pods
image::images/openshift_pod.png[]

Podとは、ホスト上に一緒にデプロイされた1つまたは複数のコンテナのことです。Pod
OpenShiftで定義、デプロイ、管理できるコンピュートリソースの最小の単位です。各PodはSDN上で独自の内部IPアドレスを割り当てられ、ポート範囲全体を所有します。Pod内のコンテナはローカルストレージ領域とネットワークリソースを共有できます。

PodはOpenShiftでは **静的な** オブジェクトとして扱われ、実行中にPodの定義を変更することはできません。

Podの一覧は次のように取得できます。

[source,bash,role="execute"]
----
oc get pods
----

以下のように出力されます。

----
NAME             READY   STATUS      RESTARTS   AGE
mapit-1-5qmgw    1/1     Running     0          24s
mapit-1-deploy   0/1     Completed   0          32s
----

NOTE: Pod名はデプロイプロセスの一部として動的に生成されますが、これについては後ほど説明します。あなたの環境とは名前は若干異なります。

NOTE: `-deploy` Podは後述する `DeploymentConfig` に関連しています。

`describe` コマンドを使うと、ポッドの詳細を知ることができます。上記のPod名の場合、

[source,bash,role="copypaste copypaste-warning"]
----
oc describe pod mapit-1-5qmgw
----

と実行すると、以下のような出力が表示されます。

----
Name:               mapit-1-5qmgw
Namespace:          app-management
Priority:           0
PriorityClassName:  <none>
Node:               ip-10-0-139-135.ec2.internal/10.0.139.135
Start Time:         Mon, 08 Apr 2019 19:38:55 +0000
Labels:             app=mapit
                    deployment=mapit-1
                    deploymentconfig=mapit
Annotations:        k8s.v1.cni.cncf.io/networks-status:
                      [{
                          "name": "openshift-sdn",
                          "interface": "eth0",
                          "ips": [
                              "10.128.4.13"
                          ],
                          "default": true,
                          "dns": {}
                      }]
                    openshift.io/deployment-config.latest-version: 1
                    openshift.io/deployment-config.name: mapit
                    openshift.io/deployment.name: mapit-1
                    openshift.io/generated-by: OpenShiftNewApp
                    openshift.io/scc: restricted
Status:             Running
IP:                 10.128.4.13
Controlled By:      ReplicationController/mapit-1
Containers:
  mapit:
    Container ID:   cri-o://77b0d785c73faf96d911b4fb7732f4f2f75666f9377e488b59bfee3fd6a5ede6
    Image:          quay.io/thoraxe/mapit@sha256:8c7e0349b6a016e3436416f3c54debda4594ba09fd34b8a0dee0c4497102590d
    Image ID:       quay.io/thoraxe/mapit@sha256:8c7e0349b6a016e3436416f3c54debda4594ba09fd34b8a0dee0c4497102590d
    Ports:          9779/TCP, 8080/TCP, 8778/TCP
    Host Ports:     0/TCP, 0/TCP, 0/TCP
    State:          Running
      Started:      Mon, 08 Apr 2019 19:39:13 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-nfwnb (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  default-token-nfwnb:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-nfwnb
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason     Age    From                                   Message
  ----    ------     ----   ----                                   -------
  Normal  Scheduled  2m16s  default-scheduler                      Successfully assigned app-management/mapit-1-5qmgw to ip-10-0-139-135.ec2.internal
  Normal  Pulling    2m7s   kubelet, ip-10-0-139-135.ec2.internal  Pulling image "quay.io/thoraxe/mapit@sha256:8c7e0349b6a016e3436416f3c54debda4594ba09fd34b8a0dee0c4497102590d"
  Normal  Pulled     118s   kubelet, ip-10-0-139-135.ec2.internal  Successfully pulled image "quay.io/thoraxe/mapit@sha256:8c7e0349b6a016e3436416f3c54debda4594ba09fd34b8a0dee0c4497102590d"
  Normal  Created    118s   kubelet, ip-10-0-139-135.ec2.internal  Created container mapit
  Normal  Started    118s   kubelet, ip-10-0-139-135.ec2.internal  Started container mapit
----

これは、実行しているPodの詳細な説明です。Podがどのノードで動いているか、ポッドの内部IPアドレス、各種ラベル、その他何が起こっているかについての情報を見ることができます。

### Services
.OpenShift Service
image::images/openshift_service.png[]

*Services* はOpenShift内部でPodのようなグループを見つけるのに便利な抽象化レイヤーを提供します。サービスはまた、それらのPodとOpenShift環境内からPodにアクセスする必要のある他の何かとの間の内部プロキシ/ロードバランサーとしても機能します。
例えば、負荷を処理するためにより多くの `mapit` インスタンスが必要な場合、より多くのPodを立ち上げることができますが、OpenShiftは自動的にそれらのPodを *Service* へのエンドポイントとしてマップします。入ってくるリクエストはこれまでと変わらず処理され、*Service* がリクエストを処理するために良い仕事をするようになったということ以外は、何も変わったことに気づかないでしょう。

OpenShiftにイメージの実行を依頼すると、`new-app` コマンドが自動的に *Service* を作成しました。ここで覚えていただきたいことは、ServiceはOpenShift内部のためのものであるということです。「外の世界」から利用することはできません。これについてはあとで学習します。

*Service* が一連のPodにマップされる方法は、 *Labels* と *Selectors* を介して行われます。 *Services* には固定IPアドレスが割り当てられ、多くのポートやプロトコルをマッピングすることができます。

手作業で作成するためのYAML形式など、
https://docs.openshift.com/container-platform/3.11/architecture/core_concepts/pods_and_services.html#services[Services]
については公式ドキュメントに多くの情報があります。

次のようにProject内のServiceのリストを見ることができます。

[source,bash,role="execute"]
----
oc get services
----

下記のように表示されます。

----
NAME      TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
mapit     ClusterIP   172.30.48.204   <none>        8080/TCP,8778/TCP,9779/TCP   3m
----

NOTE: ServiceのIPアドレスは、作成時に動的に割り当てられ、不変です。ServiceのIPアドレスはは変更されることはなく、Serviceが削除されるまで予約されます。あなたのIPアドレスは異なる可能性があります。

Podと同じように、Serviceも `describe` できます。OpenShiftではほとんどのオブジェクトを `describe` することができます。

[source,bash,role="execute"]
----
oc describe service mapit
----

以下のようなものが表示されます。

----
Name:              mapit
Namespace:         app-management
Labels:            app=mapit
Annotations:       openshift.io/generated-by: OpenShiftNewApp
Selector:          app=mapit,deploymentconfig=mapit
Type:              ClusterIP
IP:                172.30.1.208
Port:              8080-tcp  8080/TCP
TargetPort:        8080/TCP
Endpoints:         10.128.4.13:8080
Port:              8778-tcp  8778/TCP
TargetPort:        8778/TCP
Endpoints:         10.128.4.13:8778
Port:              9779-tcp  9779/TCP
TargetPort:        9779/TCP
Endpoints:         10.128.4.13:9779
Session Affinity:  None
Events:            <none>
----

すべてのオブジェクトに関する情報(それらの定義、オブジェクトの状態など)は、etcdデータストアに格納されます。etcdはデータをKeyとValueのペアとして格納し、このデータはすべてシリアライズ可能なデータオブジェクト（JSON、YAML）として表すことができます。

ServiceのYAML出力を見てみましょう。

[source,bash,role="execute"]
----
oc get service mapit -o yaml
----

以下のようなものが表示されます。

----
apiVersion: v1
kind: Service
metadata:
  annotations:
    openshift.io/generated-by: OpenShiftNewApp
  creationTimestamp: 2019-04-08T19:38:45Z
  labels:
    app: mapit
  name: mapit
  namespace: app-management
  resourceVersion: "189058"
  selfLink: /api/v1/namespaces/app-management/services/mapit
  uid: ec6ab96f-5a35-11e9-97f0-0a1014b36356
spec:
  clusterIP: 172.30.1.208
  ports:
  - name: 8080-tcp
    port: 8080
    protocol: TCP
    targetPort: 8080
  - name: 8778-tcp
    port: 8778
    protocol: TCP
    targetPort: 8778
  - name: 9779-tcp
    port: 9779
    protocol: TCP
    targetPort: 9779
  selector:
    app: mapit
    deploymentconfig: mapit
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
----

`selector` スタンザに注目し、これを覚えておきましょう。

また、`Pod` のYAMLを見て、OpenShiftがコンポーネントをどのように繋いでいるかを理解するのも面白いことです。戻って `mapit` Podの名前を探して、以下を実行します。

[source,bash,role="copypaste copypaste-warning"]
----
oc get pod mapit-1-5qmgw -o yaml
----

`metadata` セクションの下に、以下のように表示されているはずです。

----
  labels:
    app: mapit
    deployment: mapit-1
    deploymentconfig: mapit
  name: mapit-1-5qmgw
----

* *Service* には `app:mapit` と `deploymentconfig: mapit` を参照する `selector` スタンザがあります。
* *Pod* には複数の *Label* があります。
** `deploymentconfig: mapit`
** `app: mapit`
** `deployment: mapit-1`

*Labels* は単なるkey/valueのペアです。この *Project* 内で *Selector* に一致する *Label* 持つ *Pod* はすべて、 *Service* 関連付けられます。もう一度 `describe` の出力を見てみると、Serviceのエンドポイントが1つあります。これはつまり、既存の `mapit` *Pod* であることがわかります。

`new-app` のデフォルトの動作は、リクエストされたアイテムのインスタンスを1つだけ作成することです。これを修正/調整する方法を見ていきますが、その前にいくつかの概念を学んでおきましょう。

### 背景: Deployment Configuration と Replication Controllers

*Service* が *Pod* のルーティングとロードバランシングを提供するのに対し、 *ReplicationControllers* (RC) は、必要な数の *Pod* (レプリカ) を確実に存在させるために使用されます。例えば、アプリケーションを常に3つの *Pod* (インスタンス) にスケールしたい場合、*ReplicationController* が必要になります。RCがないと、何らかの理由で終了した *Pod* は自動的に再起動されません。 *ReplicationController* はOpenShiftが「自己修復」する方法です。

*DeploymentConfiguration* (DC) はOpenShift内の何かをどのようにデプロイするかを定義します。From the https://docs.openshift.com/container-platform/3.11/architecture/core_concepts/deployments.html[deployments documentation^]:

----
Building on replication controllers, OpenShift adds expanded support for the
software development and deployment lifecycle with the concept of deployments.
In the simplest case, a deployment just creates a new replication controller and
lets it start up pods. However, OpenShift deployments also provide the ability
to transition from an existing deployment of an image to a new one and also
define hooks to be run before or after creating the replication controller.
----

ほとんどの場合、*Pod* , *Service* , *ReplicationController* , *DeploymentConfiguration* リソースを一緒に使用することになります。そして、ほとんどの場合、OpenShiftがすべてのリソースを作成してくれます。

*DC* や *Service* がない *Pod* や *RC* が求められるエッジケースもありますが、これらはこの演習では説明しない高度なトピックです。

### Deploymentに関連するオブジェクトの探索

*ReplicatonController* と *DeploymentConfig* が何であるかの背景が分かったので、それらがどのように動作し、どのように関連しているかを探ってみましょう。OpenShiftに `mapit` イメージを立ち上げるように指示したときに作成された *DeploymentConfig* (DC) を見てみましょう。

[source,bash,role="execute"]
----
oc get dc
----

以下のように表示されます。

----
NAME      REVISION   DESIRED   CURRENT   TRIGGERED BY
mapit     1          1         1         config,image(mapit:latest)
----

より詳しく知るために、*ReplicationController* (RC) について調べることができます。

OpenShiftに `mapit` イメージを立ち上げるように指示したときに作成された *ReplicationController* (RC) を見てみましょう。

[source,bash,role="execute"]
----
oc get rc
----

以下のようなものが表示されます。

----
NAME      DESIRED   CURRENT   READY     AGE
mapit-1   1         1         1         4h
----

これより、1つの *Pod* がデプロイされることが希望され (Desired)、実際にデプロイされた *Pod* が1つあることがわかります (Current)。希望の数を変更することで、OpenShiftに *Pod* の数を増やしたいか減らしたいかを伝えることができます。

### Scaling the Application

Let's scale our mapit "application" up to 2 instances. We can do this with
the `scale` command.

[source,bash,role="execute"]
----
oc scale --replicas=2 dc/mapit
----

To verify that we changed the number of replicas, issue the following command:

[source,bash,role="execute"]
----
oc get rc
----

You will see something like the following:

----
NAME         DESIRED   CURRENT   READY     AGE
mapit-1      2         2         1         4h
----

You can see that we now have 2 replicas. Let's verify the number of pods with
the `oc get pods` command:

[source,bash,role="execute"]
----
oc get pods
----

You will see something like the following:

----
NAME            READY     STATUS    RESTARTS   AGE
mapit-1-6lczv   1/1       Running   0          4h
mapit-1-rq6t6   1/1       Running   0          1m
----

And lastly, let's verify that the *Service* that we learned about in the
previous lab accurately reflects two endpoints:

[source,bash,role="execute"]
----
oc describe svc mapit
----

You will see something like the following:

----
Name:              mapit
Namespace:         app-management
Labels:            app=mapit
Annotations:       openshift.io/generated-by=OpenShiftNewApp
Selector:          app=mapit,deploymentconfig=mapit
Type:              ClusterIP
IP:                172.30.48.204
Port:              8080-tcp  8080/TCP
TargetPort:        8080/TCP
Endpoints:         10.129.0.2:8080,10.130.0.3:8080
Port:              8778-tcp  8778/TCP
TargetPort:        8778/TCP
Endpoints:         10.129.0.2:8778,10.130.0.3:8778
Port:              9779-tcp  9779/TCP
TargetPort:        9779/TCP
Endpoints:         10.129.0.2:9779,10.130.0.3:9779
Session Affinity:  None
Events:            <none>

----

Another way to look at a *Service*'s endpoints is with the following:

[source,bash,role="execute"]
----
oc get endpoints mapit
----

And you will see something like the following:

----
NAME      ENDPOINTS                                                     AGE
mapit     10.128.2.3:9779,10.129.0.3:9779,10.128.2.3:8080 + 3 more...   4h
----

Your IP addresses will likely be different, as each pod receives a unique IP
within the OpenShift environment. The endpoint list is a quick way to see how
many pods are behind a service.

Overall, that's how simple it is to scale an application (*Pods* in a
*Service*). Application scaling can happen extremely quickly because OpenShift
is just launching new instances of an existing image, especially if that image
is already cached on the node.

One last thing to note is that there are actually several ports defined on this
*Service*. Earlier we said that a pod gets a single IP and has control of the
entire port space on that IP. While something running inside the *Pod* may listen
on multiple ports (single container using multiple ports, individual containers
using individual ports, a mix), a *Service* can actually proxy/map ports to
different places.

For example, a *Service* could listen on port 80 (for legacy reasons) but the
*Pod* could be listening on port 8080, 8888, or anything else.

In this `mapit` case, the image we ran has several `EXPOSE` statements in the
`Dockerfile`, so OpenShift automatically created ports on the service and mapped
them into the *Pods*.

### Application "Self Healing"

Because OpenShift's *RCs* are constantly monitoring to see that the desired number
of *Pods* are actually running, you might also expect that OpenShift will "fix" the
situation if it is ever not right. You would be correct!

Since we have two *Pods* running right now, let's see what happens if we
"accidentally" kill one. Run the `oc get pods` command again, and choose a *Pod*
name. Then, do the following:

[source,bash,role="copypaste copypaste-warning"]
----
oc delete pod mapit-1-6lczv && oc get pods
----

And you will see something like the following:

----
pod "mapit-1-lhqgq" deleted
NAME            READY     STATUS              RESTARTS   AGE
mapit-1-7dw5t   1/1       Running             0          3m
mapit-1-rgnht   0/1       ContainerCreating   0          2s
----

Did you notice anything? There's a new container already being created.

Also, the `ContainerCreating` *Pod* has a different name. That's because
OpenShift almost immediately detected that the current state (1 *Pod*,
because one was deleted) didn't match the desired state (2 *Pods*), and it
fixed it by scheduling another *Pod*.

### Background: Routes
.OpenShift Route
image::images/openshift_route.png[]

While *Services* provide internal abstraction and load balancing within an
OpenShift environment, sometimes clients (users, systems, devices, etc.)
**outside** of OpenShift need to access an application. The way that external
clients are able to access applications running in OpenShift is through the
OpenShift routing layer. And the data object behind that is a *Route*.

The default OpenShift router (HAProxy) uses the HTTP header of the incoming
request to determine where to proxy the connection. You can optionally define
security, such as TLS, for the *Route*. If you want your *Services* (and by
extension, your *Pods*) to be accessible to the outside world, then you need to
create a *Route*.

Do you remember setting up the router? You probably don't. That's because the
installation deployed an Operator for the router, and the operator created a
router for you! The router lives in the `openshift-ingress`
*Project*, and you can see information about it with the following command:

[source,bash,role="execute"]
----
oc describe deployment router-default -n openshift-ingress
----

NOTE: A *Deployment* is a Kubernetes native object, whereas a
*DeploymentConfig* is an OpenShift-specific object with a few extra features,
namely around `triggers` and what caues new deployments to take place.

You will explore the Operator for the router more in a subsequent exercise.

### Creating a Route
Creating a *Route* is a pretty straight-forward process.  You simply `expose`
the *Service* via the command line. If you remember from earlier, your *Service*
name is `mapit`. With the *Service* name, creating a *Route* is a simple
one-command task:

[source,bash,role="execute"]
----
oc expose service mapit
----

You will see:

----
route.route.openshift.io/mapit exposed
----

Verify the *Route* was created with the following command:

[source,bash,role="execute"]
----
oc get route
----

You will see something like:

----
NAME    HOST/PORT                                                           PATH   SERVICES   PORT       TERMINATION   WILDCARD
mapit   mapit-app-management.{{ ROUTE_SUBDOMAIN }}              mapit      8080-tcp
----

If you take a look at the `HOST/PORT` column, you'll see a familiar looking
FQDN. The default behavior of OpenShift is to expose services on a formulaic
hostname:

`{SERVICENAME}-{PROJECTNAME}.{ROUTINGSUBDOMAIN}`

In the subsequent router Operator labs we'll explore this and other
configuration options.

While the router configuration specifies the domain(s) that the router should
listen for, something still needs to get requests for those domains to the
Router in the first place. There is a wildcard DNS entry that points
`+*.apps...+` to the host where the router lives. OpenShift concatenates the
*Service* name, *Project* name, and the routing subdomain to create this
FQDN/URL.

You can visit this URL using your browser, or using `curl`, or any other tool.
It should be accessible from anywhere on the internet.

The *Route* is associated with the *Service*, and the router automatically
proxies connections directly to the *Pod*. The router itself runs as a *Pod*. It
bridges the "real" internet to the SDN.

If you take a step back to examine everything you've done so far, in three
commands you deployed an application, scaled it, and made it accessible to the
outside world:

----
oc new-app quay.io/thoraxe/mapit
oc scale --replicas=2 dc/mapit
oc expose service mapit
----

### Scale Down
Before we continue, go ahead and scale your application down to a single
instance:

[source,bash,role="execute"]
----
oc scale --replicas=1 dc/mapit
----

### Application Probes
OpenShift provides rudimentary capabilities around checking the liveness and/or
readiness of application instances. If the basic checks are insufficient,
OpenShift also allows you to run a command inside the *Pod*/container in order
to perform the check. That command could be a complicated script that uses any
language already installed inside the container image.

There are two types of application probes that can be defined:

*Liveness Probe*

A liveness probe checks if the container in which it is configured is still
running. If the liveness probe fails, the container is killed, which will be
subjected to its restart policy.

*Readiness Probe*

A readiness probe determines if a container is ready to service requests. If the
readiness probe fails, the endpoint's controller ensures the container has its IP
address removed from the endpoints of all services that should match it. A
readiness probe can be used to signal to the endpoint's controller that even
though a container is running, it should not receive any traffic.

More information on probing applications is available in the
https://docs.openshift.com/container-platform/4.2/nodes/containers/nodes-containers-health.html[Application
Health] section of the documentation.

### Add Probes to the Application
The `oc set` command can be used to perform several different functions, one of
which is creating and/or modifying probes. The `mapit` application exposes an
endpoint which we can check to see if it is alive and ready to respond. You can
test it using `curl`:

[source,bash,role="execute"]
----
curl mapit-app-management.{{ ROUTE_SUBDOMAIN }}/health
----

You will get some JSON as a response:

[source,json]
----
{"status":"UP","diskSpace":{"status":"UP","total":10724835328,"free":10257825792,"threshold":10485760}}
----

We can ask OpenShift to probe this endpoint for liveness with the following
command:

[source,bash,role="execute"]
----
oc set probe dc/mapit --liveness --get-url=http://:8080/health --initial-delay-seconds=30
----

You can then see that this probe is defined in the `oc describe` output:

[source,bash,role="execute"]
----
oc describe dc mapit
----

You will see a section like:

----
...
  Containers:
   mapit:
    Image:		quay.io/thoraxe/mapit@sha256:8c7e0349b6a016e3436416f3c54debda4594ba09fd34b8a0dee0c4497102590d
    Ports:		9779/TCP, 8080/TCP, 8778/TCP
    Host Ports:		0/TCP, 0/TCP, 0/TCP
    Liveness:		http-get http://:8080/health delay=30s timeout=1s period=10s #success=1 #failure=3
    Environment:	<none>
    Mounts:		<none>
  Volumes:		<none>
...
----

Similarly, you can set a readiness probe in the same manner:

[source,bash,role="execute"]
----
oc set probe dc/mapit --readiness --get-url=http://:8080/health --initial-delay-seconds=30
----

### Examining DeploymentConfigs and ReplicationControllers

Execute the following:

[source,bash,role="execute"]
----
oc get pods
----

You should see something like:

----
NAME             READY   STATUS      RESTARTS   AGE
mapit-1-deploy   0/1     Completed   0          18h
mapit-2-deploy   0/1     Completed   0          112s
mapit-3-deploy   0/1     Completed   0          75s
mapit-3-kkwxq    1/1     Running     0          66s
----

Notice that there are 3 `-deploy` pods? And that the name of the current
`mapit` pod has the number 3 in it? This is because each change to the
*DeploymentConfig* is counted as a _configuration_ change, which _triggered_ a
new _deployment_. The `-deploy` pod is responsible for ensuring the new
deployment happens.

Execute the following:

[source,bash,role="execute"]
----
oc get deploymentconfigs
----

You should see something like:

----
NAME    REVISION   DESIRED   CURRENT   TRIGGERED BY
mapit   3          1         1         config,image(mapit:latest)
----

You made two material configuration changes after the initial deployment,
thus you are now on the third revision of the *DeploymentConfiguration*.

Execute the following:

[source,bash,role="execute"]
----
oc get replicationcontrollers
----

You should see something like:

----
NAME      DESIRED   CURRENT   READY   AGE
mapit-1   0         0         0       18h
mapit-2   0         0         0       5m14s
mapit-3   1         1         1       4m37s
----

Each time a new deployment is triggered, the deployer pod creates a new
*ReplicationController* which then is responsible for ensuring that pods
exist. Notice that the old RCs have a desired scale of zero, and the most
recent RC has a desired scale of 1.

If you `oc describe` each of these RCs you will see how `-1` has no probes,
and then `-2` and `-3` have the new probes, respectively.
